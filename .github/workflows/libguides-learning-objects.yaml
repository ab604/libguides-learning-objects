# Workflow to wrangle and summarise the LibGuides Learning objects
name: LibGuides Learning Objects

# Define when the workflow should run
on:
  schedule:
    #- cron: '0 1 15 1-12 *'  # Run once a month on the 15th day of the month at 0100 UTC
  workflow_dispatch:  # Allow manual triggering of the workflow

# Set explicit permissions for the workflow
permissions:
    contents: write  # Allow writing to repository contents
    pages: write
    id-token: write

# Allow only one concurrent deployment
concurrency:
  group: "pages"
  cancel-in-progress: true

jobs:
  learning_objects:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
    # Check out the repository code
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}  # Authenticate using GitHub token
    - name: Run Keepalive
      uses: gautamkrishnar/keepalive-workflow@v2 # using the workflow with default settings

    # Set up Python environment
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.x'

    # Cache pip packages to speed up future runs
    - name: Cache pip packages
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    # # Cache Playwright browsers to avoid reinstalling
    # - name: Cache Playwright browsers
    #   uses: actions/cache@v4
    #   with:
    #     path: ~/.cache/ms-playwright
    #     key: ${{ runner.os }}-playwright-${{ hashFiles('**/package-lock.json') }}
    #     restore-keys: |
    #       ${{ runner.os }}-playwright-

    # Install required Python packages oauthlib.oauth2
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas requests
        
    # Run the data wrangling script
    - name: Run Learning Object Wrangle
      run: python learning-object-data.py

    - name: Create status message
      id: status_message
      run: |
          echo "STATUS_MESSAGE=LibGuides Learning Objects Webpage has updated. For more information, please visit: [https://ab604.github.io/link-crawler/](https://ab604.github.io/libguides-learning-objects/)." >> $GITHUB_ENV

    # - name: Send email notification
      # id: send_email
      # if: always()
      # uses: dawidd6/action-send-mail@v3
      # with:
        # server_address: smtp.gmail.com
        # server_port: 465
        # username: ${{secrets.GMAIL_USERNAME}}
        # password: ${{secrets.GMAIL_PASSWORD}}
        # subject: ${{ steps.check_broken_links.outputs.broken_links_found == 'true' && 'FAO Carrie - Broken Links Found on Library Website' || 'FAO Carrie - No Broken Links Found on Library Website' }}
        # html_body: ${{ env.STATUS_MESSAGE }}
        # convert_markdown: true
        # attachments: ${{ env.ATTACHMENT }}
        # to: ${{secrets.LIB_EMAIL_RECIPIENT}}
        # cc: ${{secrets.EMAIL_RECIPIENT}}, ${{secrets.AB_EMAIL_RECIPIENT}}
        # from: Library Linkchecker
        # priority: normal
        # secure: true

    # Prepare website directory for deployment
    - name: Prepare website artifacts
      if: always()
      run: |
        # Create output directory
        mkdir -p _site
        mkdir -p _site/tables

        # Copy essential files
        cp index.html _site/
        cp black-stag.svg _site/ # Add the logo
        cp white-stag.svg _site/
        cp tables/*.csv _site/tables/
        
    # Setup and deploy to GitHub Pages
    - name: Setup Pages
      uses: actions/configure-pages@v4

    - name: Upload artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: '_site'

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4
